## RNN
<image src="Figs/rnn/01_rnn.png" height="250px">
<image src="Figs/rnn/02_rnn.png" height="250px">
<image src="Figs/rnn/03_rnn.png" height="250px">
<image src="Figs/rnn/04_rnn.png" height="250px">
<image src="Figs/rnn/05_rnn.png" height="250px">
<image src="Figs/rnn/06_rnn_cell.png" height="250px">
<image src="Figs/rnn/07_lstm_cell.png" height="250px">


## LSTM
http://colah.github.io/posts/2015-08-Understanding-LSTMs/

A special kind of RNN, capable of learning long-term dependencies.

In the last few years, there have been incredible success applying RNNs to a variety of problems: 
speech recognition, language modeling, translation, image captioning.
The list goes on. 
The amazing feats one can achieve with RNNs.
( Andrej Karpathy’s excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks)

One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, 
such as using previous video frames might inform the understanding of the present frame. 
If RNNs could do this, they’d be extremely useful. But can they? It depends.

It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.

## Readings
[![CS231n Lecture 10 - Recurrent Neural Networks, Image Captioning, LSTM](https://img.youtube.com/vi/v=iX5V1WpxxkY/0.jpg)](https://www.youtube.com/watch?v=iX5V1WpxxkY)


