{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Skip-gram word2vec\n",
    "\n",
    "In this notebook, I'll lead you through using TensorFlow to implement the word2vec algorithm using the skip-gram architecture. By implementing this, you'll learn about embedding words for use in natural language processing. This will come in handy when dealing with things like translations.\n",
    "\n",
    "## Readings\n",
    "\n",
    "Here are the resources I used to build this notebook. I suggest reading these either beforehand or while you're working on this material.\n",
    "\n",
    "* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of word2vec from Chris McCormick \n",
    "* [First word2vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
    "* [NIPS paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for word2vec also from Mikolov et al.\n",
    "* An [implementation of word2vec](http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/) from Thushan Ganegedara\n",
    "* TensorFlow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "When you're dealing with language and words, you end up with tens of thousands of classes to predict, one for each word. Trying to one-hot encode these words is massively inefficient, you'll have one element set to 1 and the other 50,000 set to 0. The word2vec algorithm finds much more efficient representations by finding vectors that represent the words. These vectors also contain semantic information about the words. Words that show up in similar contexts, such as \"black\", \"white\", and \"red\" will have vectors near each other. There are two architectures for implementing word2vec, CBOW (Continuous Bag-Of-Words) and Skip-gram.\n",
    "\n",
    "<img src=\"assets/word2vec_architectures.png\" width=\"500\">\n",
    "\n",
    "In this implementation, we'll be using the skip-gram architecture because it performs better than CBOW. Here, we pass in a word and try to predict the words surrounding it in the text. In this way, we can train the network to learn representations for words that show up in similar contexts.\n",
    "\n",
    "First up, importing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [text8 dataset](http://mattmahoney.net/dc/textdata.html), a file of cleaned up Wikipedia articles from Matt Mahoney. The next cell will download the data set to the `data` folder. Then you can extract it and delete the archive file to save storage space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "dataset_folder_path = 'data'\n",
    "dataset_filename = 'text8.zip'\n",
    "dataset_name = 'Text8 Dataset'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(dataset_filename):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset_name) as pbar:\n",
    "        urlretrieve(\n",
    "            'http://mattmahoney.net/dc/text8.zip',\n",
    "            dataset_filename,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with zipfile.ZipFile(dataset_filename) as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder_path)\n",
    "        \n",
    "with open('data/text8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here I'm fixing up the text to make training easier. This comes from the `utils` module I wrote. The `preprocess` function coverts any punctuation into tokens, so a period is changed to ` <PERIOD> `. In this data set, there aren't any periods, but it will help in other NLP problems. I'm also _**removing all words that show up five or fewer times**_ in the dataset. This will greatly reduce issues due to noise in the data and improve the quality of the vector representations. If you want to write your own functions for this stuff, go for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
     ]
    }
   ],
   "source": [
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'platform', 'access', 'to', 'the', 'fortress', 'the', 'site', 'of', 'masada', 'was', 'identified', 'in', 'one', 'eight', 'four', 'two', 'and', 'extensively', 'excavated', 'in', 'one', 'nine', 'six', 'three', 'one', 'nine', 'six', 'five', 'b']\n"
     ]
    }
   ],
   "source": [
    "print(words[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 16680599\n",
      "Unique words: 63641\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here I'm creating dictionaries to covert words to integers and backwards, integers to words. The integers are assigned in descending frequency order, so the most frequent word (\"the\") is given the integer 0 and the next most frequent is 1 and so on. The words are converted to integers and stored in the list `int_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'the',\n",
       " 1: 'of',\n",
       " 2: 'and',\n",
       " 3: 'one',\n",
       " 4: 'in',\n",
       " 5: 'a',\n",
       " 6: 'to',\n",
       " 7: 'zero',\n",
       " 8: 'nine',\n",
       " 9: 'two',\n",
       " 10: 'is',\n",
       " 11: 'as',\n",
       " 12: 'eight',\n",
       " 13: 'for',\n",
       " 14: 's',\n",
       " 15: 'five',\n",
       " 16: 'three',\n",
       " 17: 'was',\n",
       " 18: 'by',\n",
       " 19: 'that',\n",
       " 20: 'four',\n",
       " 21: 'six',\n",
       " 22: 'seven',\n",
       " 23: 'with',\n",
       " 24: 'on',\n",
       " 25: 'are',\n",
       " 26: 'it',\n",
       " 27: 'from',\n",
       " 28: 'or',\n",
       " 29: 'his',\n",
       " 30: 'an',\n",
       " 31: 'be',\n",
       " 32: 'this',\n",
       " 33: 'which',\n",
       " 34: 'at',\n",
       " 35: 'he',\n",
       " 36: 'also',\n",
       " 37: 'not',\n",
       " 38: 'have',\n",
       " 39: 'were',\n",
       " 40: 'has',\n",
       " 41: 'but',\n",
       " 42: 'other',\n",
       " 43: 'their',\n",
       " 44: 'its',\n",
       " 45: 'first',\n",
       " 46: 'they',\n",
       " 47: 'some',\n",
       " 48: 'had',\n",
       " 49: 'all',\n",
       " 50: 'more',\n",
       " 51: 'most',\n",
       " 52: 'can',\n",
       " 53: 'been',\n",
       " 54: 'such',\n",
       " 55: 'many',\n",
       " 56: 'who',\n",
       " 57: 'new',\n",
       " 58: 'used',\n",
       " 59: 'there',\n",
       " 60: 'after',\n",
       " 61: 'when',\n",
       " 62: 'into',\n",
       " 63: 'american',\n",
       " 64: 'time',\n",
       " 65: 'these',\n",
       " 66: 'only',\n",
       " 67: 'see',\n",
       " 68: 'may',\n",
       " 69: 'than',\n",
       " 70: 'world',\n",
       " 71: 'i',\n",
       " 72: 'b',\n",
       " 73: 'would',\n",
       " 74: 'd',\n",
       " 75: 'no',\n",
       " 76: 'however',\n",
       " 77: 'between',\n",
       " 78: 'about',\n",
       " 79: 'over',\n",
       " 80: 'years',\n",
       " 81: 'states',\n",
       " 82: 'people',\n",
       " 83: 'war',\n",
       " 84: 'during',\n",
       " 85: 'united',\n",
       " 86: 'known',\n",
       " 87: 'if',\n",
       " 88: 'called',\n",
       " 89: 'use',\n",
       " 90: 'th',\n",
       " 91: 'system',\n",
       " 92: 'often',\n",
       " 93: 'state',\n",
       " 94: 'so',\n",
       " 95: 'history',\n",
       " 96: 'will',\n",
       " 97: 'up',\n",
       " 98: 'while',\n",
       " 99: 'where',\n",
       " 100: 'city',\n",
       " 101: 'being',\n",
       " 102: 'english',\n",
       " 103: 'then',\n",
       " 104: 'any',\n",
       " 105: 'both',\n",
       " 106: 'under',\n",
       " 107: 'out',\n",
       " 108: 'made',\n",
       " 109: 'well',\n",
       " 110: 'her',\n",
       " 111: 'e',\n",
       " 112: 'number',\n",
       " 113: 'government',\n",
       " 114: 'them',\n",
       " 115: 'm',\n",
       " 116: 'later',\n",
       " 117: 'since',\n",
       " 118: 'him',\n",
       " 119: 'part',\n",
       " 120: 'name',\n",
       " 121: 'c',\n",
       " 122: 'century',\n",
       " 123: 'through',\n",
       " 124: 'because',\n",
       " 125: 'x',\n",
       " 126: 'university',\n",
       " 127: 'early',\n",
       " 128: 'life',\n",
       " 129: 'british',\n",
       " 130: 'year',\n",
       " 131: 'like',\n",
       " 132: 'same',\n",
       " 133: 'including',\n",
       " 134: 'became',\n",
       " 135: 'example',\n",
       " 136: 'day',\n",
       " 137: 'each',\n",
       " 138: 'even',\n",
       " 139: 'work',\n",
       " 140: 'language',\n",
       " 141: 'although',\n",
       " 142: 'several',\n",
       " 143: 'form',\n",
       " 144: 'john',\n",
       " 145: 'u',\n",
       " 146: 'national',\n",
       " 147: 'very',\n",
       " 148: 'much',\n",
       " 149: 'g',\n",
       " 150: 'french',\n",
       " 151: 'before',\n",
       " 152: 'general',\n",
       " 153: 'what',\n",
       " 154: 't',\n",
       " 155: 'against',\n",
       " 156: 'n',\n",
       " 157: 'high',\n",
       " 158: 'links',\n",
       " 159: 'could',\n",
       " 160: 'based',\n",
       " 161: 'those',\n",
       " 162: 'now',\n",
       " 163: 'second',\n",
       " 164: 'de',\n",
       " 165: 'music',\n",
       " 166: 'another',\n",
       " 167: 'large',\n",
       " 168: 'she',\n",
       " 169: 'f',\n",
       " 170: 'external',\n",
       " 171: 'german',\n",
       " 172: 'different',\n",
       " 173: 'modern',\n",
       " 174: 'great',\n",
       " 175: 'do',\n",
       " 176: 'common',\n",
       " 177: 'set',\n",
       " 178: 'list',\n",
       " 179: 'south',\n",
       " 180: 'series',\n",
       " 181: 'major',\n",
       " 182: 'game',\n",
       " 183: 'power',\n",
       " 184: 'long',\n",
       " 185: 'country',\n",
       " 186: 'king',\n",
       " 187: 'law',\n",
       " 188: 'group',\n",
       " 189: 'film',\n",
       " 190: 'still',\n",
       " 191: 'until',\n",
       " 192: 'north',\n",
       " 193: 'international',\n",
       " 194: 'term',\n",
       " 195: 'we',\n",
       " 196: 'end',\n",
       " 197: 'book',\n",
       " 198: 'found',\n",
       " 199: 'own',\n",
       " 200: 'political',\n",
       " 201: 'party',\n",
       " 202: 'order',\n",
       " 203: 'usually',\n",
       " 204: 'president',\n",
       " 205: 'church',\n",
       " 206: 'you',\n",
       " 207: 'death',\n",
       " 208: 'theory',\n",
       " 209: 'area',\n",
       " 210: 'around',\n",
       " 211: 'include',\n",
       " 212: 'god',\n",
       " 213: 'ii',\n",
       " 214: 'way',\n",
       " 215: 'did',\n",
       " 216: 'military',\n",
       " 217: 'population',\n",
       " 218: 'using',\n",
       " 219: 'though',\n",
       " 220: 'small',\n",
       " 221: 'following',\n",
       " 222: 'within',\n",
       " 223: 'non',\n",
       " 224: 'human',\n",
       " 225: 'left',\n",
       " 226: 'main',\n",
       " 227: 'among',\n",
       " 228: 'point',\n",
       " 229: 'r',\n",
       " 230: 'due',\n",
       " 231: 'p',\n",
       " 232: 'considered',\n",
       " 233: 'public',\n",
       " 234: 'popular',\n",
       " 235: 'computer',\n",
       " 236: 'west',\n",
       " 237: 'family',\n",
       " 238: 'east',\n",
       " 239: 'information',\n",
       " 240: 'important',\n",
       " 241: 'european',\n",
       " 242: 'man',\n",
       " 243: 'sometimes',\n",
       " 244: 'right',\n",
       " 245: 'old',\n",
       " 246: 'free',\n",
       " 247: 'word',\n",
       " 248: 'without',\n",
       " 249: 'last',\n",
       " 250: 'us',\n",
       " 251: 'members',\n",
       " 252: 'given',\n",
       " 253: 'times',\n",
       " 254: 'roman',\n",
       " 255: 'make',\n",
       " 256: 'h',\n",
       " 257: 'age',\n",
       " 258: 'place',\n",
       " 259: 'l',\n",
       " 260: 'thus',\n",
       " 261: 'science',\n",
       " 262: 'case',\n",
       " 263: 'become',\n",
       " 264: 'systems',\n",
       " 265: 'union',\n",
       " 266: 'born',\n",
       " 267: 'york',\n",
       " 268: 'line',\n",
       " 269: 'countries',\n",
       " 270: 'does',\n",
       " 271: 'isbn',\n",
       " 272: 'st',\n",
       " 273: 'control',\n",
       " 274: 'various',\n",
       " 275: 'others',\n",
       " 276: 'house',\n",
       " 277: 'article',\n",
       " 278: 'island',\n",
       " 279: 'should',\n",
       " 280: 'led',\n",
       " 281: 'back',\n",
       " 282: 'period',\n",
       " 283: 'player',\n",
       " 284: 'europe',\n",
       " 285: 'languages',\n",
       " 286: 'central',\n",
       " 287: 'water',\n",
       " 288: 'few',\n",
       " 289: 'western',\n",
       " 290: 'home',\n",
       " 291: 'began',\n",
       " 292: 'generally',\n",
       " 293: 'less',\n",
       " 294: 'k',\n",
       " 295: 'similar',\n",
       " 296: 'written',\n",
       " 297: 'original',\n",
       " 298: 'best',\n",
       " 299: 'must',\n",
       " 300: 'according',\n",
       " 301: 'school',\n",
       " 302: 'france',\n",
       " 303: 'air',\n",
       " 304: 'single',\n",
       " 305: 'force',\n",
       " 306: 'v',\n",
       " 307: 'land',\n",
       " 308: 'groups',\n",
       " 309: 'down',\n",
       " 310: 'how',\n",
       " 311: 'works',\n",
       " 312: 'development',\n",
       " 313: 'official',\n",
       " 314: 'support',\n",
       " 315: 'england',\n",
       " 316: 'j',\n",
       " 317: 'rather',\n",
       " 318: 'data',\n",
       " 319: 'space',\n",
       " 320: 'greek',\n",
       " 321: 'km',\n",
       " 322: 'named',\n",
       " 323: 'germany',\n",
       " 324: 'just',\n",
       " 325: 'games',\n",
       " 326: 'said',\n",
       " 327: 'version',\n",
       " 328: 'late',\n",
       " 329: 'earth',\n",
       " 330: 'company',\n",
       " 331: 'every',\n",
       " 332: 'economic',\n",
       " 333: 'short',\n",
       " 334: 'published',\n",
       " 335: 'black',\n",
       " 336: 'army',\n",
       " 337: 'off',\n",
       " 338: 'london',\n",
       " 339: 'million',\n",
       " 340: 'body',\n",
       " 341: 'field',\n",
       " 342: 'christian',\n",
       " 343: 'either',\n",
       " 344: 'social',\n",
       " 345: 'empire',\n",
       " 346: 'o',\n",
       " 347: 'developed',\n",
       " 348: 'standard',\n",
       " 349: 'court',\n",
       " 350: 'service',\n",
       " 351: 'kingdom',\n",
       " 352: 'along',\n",
       " 353: 'college',\n",
       " 354: 'republic',\n",
       " 355: 'sea',\n",
       " 356: 'america',\n",
       " 357: 'today',\n",
       " 358: 'result',\n",
       " 359: 'held',\n",
       " 360: 'team',\n",
       " 361: 'light',\n",
       " 362: 'means',\n",
       " 363: 'never',\n",
       " 364: 'especially',\n",
       " 365: 'third',\n",
       " 366: 'further',\n",
       " 367: 'forces',\n",
       " 368: 'character',\n",
       " 369: 'take',\n",
       " 370: 'men',\n",
       " 371: 'society',\n",
       " 372: 'show',\n",
       " 373: 'open',\n",
       " 374: 'possible',\n",
       " 375: 'fact',\n",
       " 376: 'battle',\n",
       " 377: 'took',\n",
       " 378: 'former',\n",
       " 379: 'books',\n",
       " 380: 'soviet',\n",
       " 381: 'river',\n",
       " 382: 'children',\n",
       " 383: 'having',\n",
       " 384: 'good',\n",
       " 385: 'local',\n",
       " 386: 'current',\n",
       " 387: 'son',\n",
       " 388: 'process',\n",
       " 389: 'natural',\n",
       " 390: 'present',\n",
       " 391: 'himself',\n",
       " 392: 'islands',\n",
       " 393: 'total',\n",
       " 394: 'near',\n",
       " 395: 'white',\n",
       " 396: 'days',\n",
       " 397: 'person',\n",
       " 398: 'itself',\n",
       " 399: 'seen',\n",
       " 400: 'culture',\n",
       " 401: 'little',\n",
       " 402: 'above',\n",
       " 403: 'software',\n",
       " 404: 'largest',\n",
       " 405: 'words',\n",
       " 406: 'upon',\n",
       " 407: 'level',\n",
       " 408: 'father',\n",
       " 409: 'created',\n",
       " 410: 'side',\n",
       " 411: 'red',\n",
       " 412: 'references',\n",
       " 413: 'press',\n",
       " 414: 'full',\n",
       " 415: 'region',\n",
       " 416: 'almost',\n",
       " 417: 'al',\n",
       " 418: 'image',\n",
       " 419: 'famous',\n",
       " 420: 'play',\n",
       " 421: 'came',\n",
       " 422: 'role',\n",
       " 423: 'once',\n",
       " 424: 'certain',\n",
       " 425: 'league',\n",
       " 426: 'jewish',\n",
       " 427: 'james',\n",
       " 428: 'january',\n",
       " 429: 'site',\n",
       " 430: 'again',\n",
       " 431: 'numbers',\n",
       " 432: 'art',\n",
       " 433: 'member',\n",
       " 434: 'areas',\n",
       " 435: 'movement',\n",
       " 436: 'religious',\n",
       " 437: 'type',\n",
       " 438: 'march',\n",
       " 439: 'community',\n",
       " 440: 'story',\n",
       " 441: 'played',\n",
       " 442: 'production',\n",
       " 443: 'released',\n",
       " 444: 'center',\n",
       " 445: 'rights',\n",
       " 446: 'real',\n",
       " 447: 'related',\n",
       " 448: 'foreign',\n",
       " 449: 'low',\n",
       " 450: 'ancient',\n",
       " 451: 'terms',\n",
       " 452: 'view',\n",
       " 453: 'source',\n",
       " 454: 'act',\n",
       " 455: 'minister',\n",
       " 456: 'change',\n",
       " 457: 'energy',\n",
       " 458: 'produced',\n",
       " 459: 'research',\n",
       " 460: 'actor',\n",
       " 461: 'making',\n",
       " 462: 'civil',\n",
       " 463: 'december',\n",
       " 464: 'women',\n",
       " 465: 'special',\n",
       " 466: 'style',\n",
       " 467: 'design',\n",
       " 468: 'japanese',\n",
       " 469: 'william',\n",
       " 470: 'available',\n",
       " 471: 'chinese',\n",
       " 472: 'forms',\n",
       " 473: 'canada',\n",
       " 474: 'northern',\n",
       " 475: 'died',\n",
       " 476: 'class',\n",
       " 477: 'living',\n",
       " 478: 'next',\n",
       " 479: 'particular',\n",
       " 480: 'program',\n",
       " 481: 'council',\n",
       " 482: 'television',\n",
       " 483: 'head',\n",
       " 484: 'david',\n",
       " 485: 'china',\n",
       " 486: 'middle',\n",
       " 487: 'established',\n",
       " 488: 'hand',\n",
       " 489: 'bc',\n",
       " 490: 'far',\n",
       " 491: 'july',\n",
       " 492: 'function',\n",
       " 493: 'position',\n",
       " 494: 'y',\n",
       " 495: 'built',\n",
       " 496: 'george',\n",
       " 497: 'band',\n",
       " 498: 'together',\n",
       " 499: 'w',\n",
       " 500: 'latin',\n",
       " 501: 'thought',\n",
       " 502: 'eastern',\n",
       " 503: 'charles',\n",
       " 504: 'parts',\n",
       " 505: 'instead',\n",
       " 506: 'study',\n",
       " 507: 'might',\n",
       " 508: 'india',\n",
       " 509: 'code',\n",
       " 510: 'included',\n",
       " 511: 'meaning',\n",
       " 512: 'trade',\n",
       " 513: 'per',\n",
       " 514: 'june',\n",
       " 515: 'least',\n",
       " 516: 'half',\n",
       " 517: 'model',\n",
       " 518: 'economy',\n",
       " 519: 'prime',\n",
       " 520: 'traditional',\n",
       " 521: 'always',\n",
       " 522: 'capital',\n",
       " 523: 'range',\n",
       " 524: 'november',\n",
       " 525: 'emperor',\n",
       " 526: 'young',\n",
       " 527: 'anti',\n",
       " 528: 'final',\n",
       " 529: 'text',\n",
       " 530: 'players',\n",
       " 531: 'uk',\n",
       " 532: 'april',\n",
       " 533: 'run',\n",
       " 534: 'september',\n",
       " 535: 'addition',\n",
       " 536: 'radio',\n",
       " 537: 'live',\n",
       " 538: 'august',\n",
       " 539: 'note',\n",
       " 540: 'taken',\n",
       " 541: 'italian',\n",
       " 542: 'lost',\n",
       " 543: 'nature',\n",
       " 544: 'project',\n",
       " 545: 'technology',\n",
       " 546: 'spanish',\n",
       " 547: 'october',\n",
       " 548: 'rate',\n",
       " 549: 'recent',\n",
       " 550: 'won',\n",
       " 551: 'true',\n",
       " 552: 'value',\n",
       " 553: 'uses',\n",
       " 554: 'russian',\n",
       " 555: 'est',\n",
       " 556: 'wrote',\n",
       " 557: 'effect',\n",
       " 558: 'album',\n",
       " 559: 'southern',\n",
       " 560: 'africa',\n",
       " 561: 'whose',\n",
       " 562: 'top',\n",
       " 563: 'historical',\n",
       " 564: 'australia',\n",
       " 565: 'catholic',\n",
       " 566: 'particularly',\n",
       " 567: 'self',\n",
       " 568: 'structure',\n",
       " 569: 'record',\n",
       " 570: 'evidence',\n",
       " 571: 'themselves',\n",
       " 572: 'rule',\n",
       " 573: 'influence',\n",
       " 574: 'cases',\n",
       " 575: 'subject',\n",
       " 576: 'referred',\n",
       " 577: 'continued',\n",
       " 578: 'nations',\n",
       " 579: 'below',\n",
       " 580: 'rock',\n",
       " 581: 'japan',\n",
       " 582: 'com',\n",
       " 583: 'song',\n",
       " 584: 'names',\n",
       " 585: 'throughout',\n",
       " 586: 'female',\n",
       " 587: 'title',\n",
       " 588: 'our',\n",
       " 589: 'therefore',\n",
       " 590: 'office',\n",
       " 591: 'star',\n",
       " 592: 'paul',\n",
       " 593: 'too',\n",
       " 594: 'cities',\n",
       " 595: 'february',\n",
       " 596: 'independent',\n",
       " 597: 'author',\n",
       " 598: 'problem',\n",
       " 599: 'species',\n",
       " 600: 'education',\n",
       " 601: 'done',\n",
       " 602: 'philosophy',\n",
       " 603: 'come',\n",
       " 604: 'higher',\n",
       " 605: 'originally',\n",
       " 606: 'market',\n",
       " 607: 'town',\n",
       " 608: 'my',\n",
       " 609: 'season',\n",
       " 610: 'love',\n",
       " 611: 'strong',\n",
       " 612: 'israel',\n",
       " 613: 'irish',\n",
       " 614: 'writer',\n",
       " 615: 'films',\n",
       " 616: 'elements',\n",
       " 617: 'robert',\n",
       " 618: 'whether',\n",
       " 619: 'despite',\n",
       " 620: 'eventually',\n",
       " 621: 'here',\n",
       " 622: 'football',\n",
       " 623: 'action',\n",
       " 624: 'internet',\n",
       " 625: 'individual',\n",
       " 626: 'sound',\n",
       " 627: 'network',\n",
       " 628: 'described',\n",
       " 629: 'practice',\n",
       " 630: 'characters',\n",
       " 631: 're',\n",
       " 632: 'royal',\n",
       " 633: 'la',\n",
       " 634: 'events',\n",
       " 635: 'formed',\n",
       " 636: 'commonly',\n",
       " 637: 'base',\n",
       " 638: 'received',\n",
       " 639: 'african',\n",
       " 640: 'problems',\n",
       " 641: 'food',\n",
       " 642: 'jews',\n",
       " 643: 'able',\n",
       " 644: 'male',\n",
       " 645: 'mass',\n",
       " 646: 'typically',\n",
       " 647: 'complex',\n",
       " 648: 'lower',\n",
       " 649: 'includes',\n",
       " 650: 'outside',\n",
       " 651: 'legal',\n",
       " 652: 'complete',\n",
       " 653: 'significant',\n",
       " 654: 'parliament',\n",
       " 655: 'actually',\n",
       " 656: 'business',\n",
       " 657: 'fiction',\n",
       " 658: 'physical',\n",
       " 659: 'followed',\n",
       " 660: 'deaths',\n",
       " 661: 'key',\n",
       " 662: 'widely',\n",
       " 663: 'leader',\n",
       " 664: 'page',\n",
       " 665: 'basic',\n",
       " 666: 'types',\n",
       " 667: 'henry',\n",
       " 668: 'elected',\n",
       " 669: 'beginning',\n",
       " 670: 'fire',\n",
       " 671: 'building',\n",
       " 672: 'independence',\n",
       " 673: 'went',\n",
       " 674: 'movie',\n",
       " 675: 'aircraft',\n",
       " 676: 'ever',\n",
       " 677: 'canadian',\n",
       " 678: 'material',\n",
       " 679: 'births',\n",
       " 680: 'video',\n",
       " 681: 'news',\n",
       " 682: 'future',\n",
       " 683: 'scientific',\n",
       " 684: 'simply',\n",
       " 685: 'go',\n",
       " 686: 'defined',\n",
       " 687: 'laws',\n",
       " 688: 'get',\n",
       " 689: 'close',\n",
       " 690: 'industry',\n",
       " 691: 'specific',\n",
       " 692: 'examples',\n",
       " 693: 'services',\n",
       " 694: 'believe',\n",
       " 695: 'idea',\n",
       " 696: 'method',\n",
       " 697: 'introduced',\n",
       " 698: 'points',\n",
       " 699: 'return',\n",
       " 700: 'cause',\n",
       " 701: 'indian',\n",
       " 702: 'britain',\n",
       " 703: 'features',\n",
       " 704: 'majority',\n",
       " 705: 'size',\n",
       " 706: 'post',\n",
       " 707: 'lead',\n",
       " 708: 'organization',\n",
       " 709: 'cannot',\n",
       " 710: 'designed',\n",
       " 711: 'ireland',\n",
       " 712: 'cross',\n",
       " 713: 'classical',\n",
       " 714: 'personal',\n",
       " 715: 'writing',\n",
       " 716: 'concept',\n",
       " 717: 'associated',\n",
       " 718: 'required',\n",
       " 719: 'soon',\n",
       " 720: 'changes',\n",
       " 721: 'located',\n",
       " 722: 'california',\n",
       " 723: 'sense',\n",
       " 724: 'believed',\n",
       " 725: 'started',\n",
       " 726: 'away',\n",
       " 727: 'co',\n",
       " 728: 'religion',\n",
       " 729: 'mother',\n",
       " 730: 'county',\n",
       " 731: 'rules',\n",
       " 732: 'studies',\n",
       " 733: 'yet',\n",
       " 734: 'find',\n",
       " 735: 'knowledge',\n",
       " 736: 'put',\n",
       " 737: 'founded',\n",
       " 738: 'policy',\n",
       " 739: 'currently',\n",
       " 740: 'provide',\n",
       " 741: 'working',\n",
       " 742: 'media',\n",
       " 743: 'election',\n",
       " 744: 'australian',\n",
       " 745: 'me',\n",
       " 746: 'thomas',\n",
       " 747: 'allowed',\n",
       " 748: 'russia',\n",
       " 749: 'earlier',\n",
       " 750: 'greater',\n",
       " 751: 'limited',\n",
       " 752: 'object',\n",
       " 753: 'brought',\n",
       " 754: 'online',\n",
       " 755: 'association',\n",
       " 756: 'lord',\n",
       " 757: 'mostly',\n",
       " 758: 'blue',\n",
       " 759: 'across',\n",
       " 760: 'constitution',\n",
       " 761: 'added',\n",
       " 762: 'interest',\n",
       " 763: 'things',\n",
       " 764: 'relations',\n",
       " 765: 'speed',\n",
       " 766: 'federal',\n",
       " 767: 'singer',\n",
       " 768: 'effects',\n",
       " 769: 'growth',\n",
       " 770: 'sources',\n",
       " 771: 'your',\n",
       " 772: 'remains',\n",
       " 773: 'z',\n",
       " 774: 'probably',\n",
       " 775: 'gave',\n",
       " 776: 'simple',\n",
       " 777: 'attack',\n",
       " 778: 'longer',\n",
       " 779: 'reference',\n",
       " 780: 'saint',\n",
       " 781: 'success',\n",
       " 782: 'killed',\n",
       " 783: 'past',\n",
       " 784: 'career',\n",
       " 785: 'need',\n",
       " 786: 'park',\n",
       " 787: 'definition',\n",
       " 788: 'say',\n",
       " 789: 'etc',\n",
       " 790: 'give',\n",
       " 791: 'peace',\n",
       " 792: 'chief',\n",
       " 793: 'stories',\n",
       " 794: 'security',\n",
       " 795: 'wide',\n",
       " 796: 'ball',\n",
       " 797: 'saw',\n",
       " 798: 'machine',\n",
       " 799: 'better',\n",
       " 800: 'cell',\n",
       " 801: 'leading',\n",
       " 802: 'becomes',\n",
       " 803: 'spain',\n",
       " 804: 'larger',\n",
       " 805: 'parties',\n",
       " 806: 'night',\n",
       " 807: 'products',\n",
       " 808: 'remained',\n",
       " 809: 'prize',\n",
       " 810: 'big',\n",
       " 811: 'months',\n",
       " 812: 'website',\n",
       " 813: 'cultural',\n",
       " 814: 'money',\n",
       " 815: 'territory',\n",
       " 816: 'help',\n",
       " 817: 'moved',\n",
       " 818: 'private',\n",
       " 819: 'wife',\n",
       " 820: 'letter',\n",
       " 821: 'lines',\n",
       " 822: 'politics',\n",
       " 823: 'largely',\n",
       " 824: 'contains',\n",
       " 825: 'companies',\n",
       " 826: 'lake',\n",
       " 827: 'perhaps',\n",
       " 828: 'green',\n",
       " 829: 'already',\n",
       " 830: 'iii',\n",
       " 831: 'dead',\n",
       " 832: 'library',\n",
       " 833: 'separate',\n",
       " 834: 'refer',\n",
       " 835: 'makes',\n",
       " 836: 'appeared',\n",
       " 837: 'dutch',\n",
       " 838: 'holy',\n",
       " 839: 'era',\n",
       " 840: 'novel',\n",
       " 841: 'successful',\n",
       " 842: 'italy',\n",
       " 843: 'letters',\n",
       " 844: 'results',\n",
       " 845: 'matter',\n",
       " 846: 'produce',\n",
       " 847: 'origin',\n",
       " 848: 'claim',\n",
       " 849: 'whole',\n",
       " 850: 'attempt',\n",
       " 851: 'directly',\n",
       " 852: 'actress',\n",
       " 853: 'surface',\n",
       " 854: 'revolution',\n",
       " 855: 'highly',\n",
       " 856: 'caused',\n",
       " 857: 'status',\n",
       " 858: 'musical',\n",
       " 859: 'richard',\n",
       " 860: 'commercial',\n",
       " 861: 'division',\n",
       " 862: 'color',\n",
       " 863: 'coast',\n",
       " 864: 'release',\n",
       " 865: 'health',\n",
       " 866: 'latter',\n",
       " 867: 'authority',\n",
       " 868: 'treaty',\n",
       " 869: 'turn',\n",
       " 870: 'michael',\n",
       " 871: 'nation',\n",
       " 872: 'direct',\n",
       " 873: 'asia',\n",
       " 874: 'edition',\n",
       " 875: 'programming',\n",
       " 876: 'playing',\n",
       " 877: 'date',\n",
       " 878: 'whom',\n",
       " 879: 'native',\n",
       " 880: 'mary',\n",
       " 881: 'married',\n",
       " 882: 'towards',\n",
       " 883: 'issues',\n",
       " 884: 'double',\n",
       " 885: 'basis',\n",
       " 886: 'primary',\n",
       " 887: 'enough',\n",
       " 888: 'allow',\n",
       " 889: 'memory',\n",
       " 890: 'reason',\n",
       " 891: 'web',\n",
       " 892: 'exist',\n",
       " 893: 'provided',\n",
       " 894: 'course',\n",
       " 895: 'functions',\n",
       " 896: 'oil',\n",
       " 897: 'chemical',\n",
       " 898: 'analysis',\n",
       " 899: 'alexander',\n",
       " 900: 'mid',\n",
       " 901: 'replaced',\n",
       " 902: 'queen',\n",
       " 903: 'tv',\n",
       " 904: 'sun',\n",
       " 905: 'claims',\n",
       " 906: 'literature',\n",
       " 907: 'metal',\n",
       " 908: 'amount',\n",
       " 909: 'divided',\n",
       " 910: 'blood',\n",
       " 911: 'likely',\n",
       " 912: 'access',\n",
       " 913: 'average',\n",
       " 914: 'length',\n",
       " 915: 'smaller',\n",
       " 916: 'medical',\n",
       " 917: 'property',\n",
       " 918: 'students',\n",
       " 919: 'degree',\n",
       " 920: 'elections',\n",
       " 921: 'club',\n",
       " 922: 'claimed',\n",
       " 923: 'performance',\n",
       " 924: 'director',\n",
       " 925: 'digital',\n",
       " 926: 'front',\n",
       " 927: 'museum',\n",
       " 928: 'difficult',\n",
       " 929: 'tradition',\n",
       " 930: 'nearly',\n",
       " 931: 'schools',\n",
       " 932: 'washington',\n",
       " 933: 'gas',\n",
       " 934: 'jesus',\n",
       " 935: 'map',\n",
       " 936: 'rome',\n",
       " 937: 'louis',\n",
       " 938: 'unit',\n",
       " 939: 'baseball',\n",
       " 940: 'mind',\n",
       " 941: 'peter',\n",
       " 942: 'mark',\n",
       " 943: 'collection',\n",
       " 944: 'product',\n",
       " 945: 'congress',\n",
       " 946: 'programs',\n",
       " 947: 'changed',\n",
       " 948: 'ideas',\n",
       " 949: 'moon',\n",
       " 950: 'entire',\n",
       " 951: 'user',\n",
       " 952: 'ground',\n",
       " 953: 'records',\n",
       " 954: 'frequently',\n",
       " 955: 'increase',\n",
       " 956: 'highest',\n",
       " 957: 'sent',\n",
       " 958: 'finally',\n",
       " 959: 'board',\n",
       " 960: 'don',\n",
       " 961: 'notable',\n",
       " 962: 'methods',\n",
       " 963: 'read',\n",
       " 964: 'recently',\n",
       " 965: 'bit',\n",
       " 966: 'variety',\n",
       " 967: 'involved',\n",
       " 968: 'call',\n",
       " 969: 'democratic',\n",
       " 970: 'ten',\n",
       " 971: 'served',\n",
       " 972: 'minor',\n",
       " 973: 'hard',\n",
       " 974: 'objects',\n",
       " 975: 'birth',\n",
       " 976: 'increased',\n",
       " 977: 'nuclear',\n",
       " 978: 'section',\n",
       " 979: 'street',\n",
       " 980: 'windows',\n",
       " 981: 'relatively',\n",
       " 982: 'car',\n",
       " 983: 'move',\n",
       " 984: 'create',\n",
       " 985: 'returned',\n",
       " 986: 'bank',\n",
       " 987: 'conditions',\n",
       " 988: 'operation',\n",
       " 989: 'adopted',\n",
       " 990: 'relationship',\n",
       " 991: 'christ',\n",
       " 992: 'hall',\n",
       " 993: 'appear',\n",
       " 994: 'rest',\n",
       " 995: 'child',\n",
       " 996: 'element',\n",
       " 997: 'appears',\n",
       " 998: 'takes',\n",
       " 999: 'fall',\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5236,\n",
       " 3082,\n",
       " 11,\n",
       " 5,\n",
       " 194,\n",
       " 1,\n",
       " 3135,\n",
       " 45,\n",
       " 58,\n",
       " 155,\n",
       " 127,\n",
       " 741,\n",
       " 476,\n",
       " 10600,\n",
       " 133,\n",
       " 0,\n",
       " 27933,\n",
       " 1,\n",
       " 0,\n",
       " 102,\n",
       " 854,\n",
       " 2,\n",
       " 0,\n",
       " 15154,\n",
       " 62840,\n",
       " 1,\n",
       " 0,\n",
       " 150,\n",
       " 854,\n",
       " 3581,\n",
       " 0,\n",
       " 194,\n",
       " 10,\n",
       " 190,\n",
       " 58,\n",
       " 4,\n",
       " 5,\n",
       " 10782,\n",
       " 214,\n",
       " 6,\n",
       " 1325,\n",
       " 104,\n",
       " 454,\n",
       " 19,\n",
       " 58,\n",
       " 2733,\n",
       " 362,\n",
       " 6,\n",
       " 3677,\n",
       " 0,\n",
       " 708,\n",
       " 1,\n",
       " 371,\n",
       " 26,\n",
       " 40,\n",
       " 36,\n",
       " 53,\n",
       " 540,\n",
       " 97,\n",
       " 11,\n",
       " 5,\n",
       " 1423,\n",
       " 2759,\n",
       " 18,\n",
       " 567,\n",
       " 686,\n",
       " 7095,\n",
       " 0,\n",
       " 247,\n",
       " 5236,\n",
       " 10,\n",
       " 1052,\n",
       " 27,\n",
       " 0,\n",
       " 320,\n",
       " 248,\n",
       " 45588,\n",
       " 2880,\n",
       " 792,\n",
       " 186,\n",
       " 5236,\n",
       " 11,\n",
       " 5,\n",
       " 200,\n",
       " 602,\n",
       " 10,\n",
       " 0,\n",
       " 1135,\n",
       " 19,\n",
       " 2623,\n",
       " 25,\n",
       " 9021,\n",
       " 2,\n",
       " 279,\n",
       " 31,\n",
       " 4147,\n",
       " 141,\n",
       " 59,\n",
       " 25,\n",
       " 6448,\n",
       " 4198,\n",
       " 1,\n",
       " 153,\n",
       " 32,\n",
       " 362,\n",
       " 5236,\n",
       " 36,\n",
       " 1137,\n",
       " 6,\n",
       " 447,\n",
       " 344,\n",
       " 1818,\n",
       " 19,\n",
       " 4871,\n",
       " 0,\n",
       " 6762,\n",
       " 1,\n",
       " 7594,\n",
       " 1775,\n",
       " 566,\n",
       " 0,\n",
       " 93,\n",
       " 0,\n",
       " 247,\n",
       " 11090,\n",
       " 11,\n",
       " 51,\n",
       " 7095,\n",
       " 89,\n",
       " 26,\n",
       " 270,\n",
       " 37,\n",
       " 5958,\n",
       " 4867,\n",
       " 20325,\n",
       " 28,\n",
       " 57859,\n",
       " 41,\n",
       " 317,\n",
       " 5,\n",
       " 25868,\n",
       " 527,\n",
       " 7594,\n",
       " 371,\n",
       " 4,\n",
       " 258,\n",
       " 1,\n",
       " 153,\n",
       " 25,\n",
       " 1206,\n",
       " 11,\n",
       " 7594,\n",
       " 200,\n",
       " 1576,\n",
       " 2,\n",
       " 15297,\n",
       " 332,\n",
       " 1775,\n",
       " 7095,\n",
       " 4871,\n",
       " 344,\n",
       " 764,\n",
       " 160,\n",
       " 406,\n",
       " 5703,\n",
       " 755,\n",
       " 1,\n",
       " 4110,\n",
       " 1131,\n",
       " 4342,\n",
       " 1536,\n",
       " 2,\n",
       " 567,\n",
       " 8129,\n",
       " 98,\n",
       " 5236,\n",
       " 10,\n",
       " 51,\n",
       " 1408,\n",
       " 686,\n",
       " 18,\n",
       " 153,\n",
       " 26,\n",
       " 10,\n",
       " 155,\n",
       " 7095,\n",
       " 36,\n",
       " 2033,\n",
       " 1423,\n",
       " 8190,\n",
       " 1,\n",
       " 153,\n",
       " 46,\n",
       " 694,\n",
       " 6,\n",
       " 31,\n",
       " 5,\n",
       " 4160,\n",
       " 246,\n",
       " 371,\n",
       " 76,\n",
       " 948,\n",
       " 78,\n",
       " 310,\n",
       " 30,\n",
       " 4780,\n",
       " 371,\n",
       " 507,\n",
       " 139,\n",
       " 2312,\n",
       " 3556,\n",
       " 364,\n",
       " 23,\n",
       " 1821,\n",
       " 6,\n",
       " 1903,\n",
       " 59,\n",
       " 10,\n",
       " 36,\n",
       " 8426,\n",
       " 78,\n",
       " 310,\n",
       " 5,\n",
       " 246,\n",
       " 371,\n",
       " 507,\n",
       " 31,\n",
       " 753,\n",
       " 78,\n",
       " 1735,\n",
       " 2,\n",
       " 8079,\n",
       " 24253,\n",
       " 2,\n",
       " 275,\n",
       " 1693,\n",
       " 19,\n",
       " 151,\n",
       " 1033,\n",
       " 95,\n",
       " 224,\n",
       " 371,\n",
       " 17,\n",
       " 1814,\n",
       " 24,\n",
       " 4780,\n",
       " 1556,\n",
       " 51,\n",
       " 8134,\n",
       " 1467,\n",
       " 24253,\n",
       " 2,\n",
       " 12800,\n",
       " 4,\n",
       " 6130,\n",
       " 19,\n",
       " 4196,\n",
       " 21164,\n",
       " 2431,\n",
       " 39,\n",
       " 16569,\n",
       " 2,\n",
       " 7247,\n",
       " 861,\n",
       " 1,\n",
       " 1194,\n",
       " 10228,\n",
       " 2515,\n",
       " 28,\n",
       " 15132,\n",
       " 187,\n",
       " 2,\n",
       " 48,\n",
       " 1123,\n",
       " 912,\n",
       " 6,\n",
       " 1049,\n",
       " 469,\n",
       " 12305,\n",
       " 7095,\n",
       " 133,\n",
       " 0,\n",
       " 0,\n",
       " 11090,\n",
       " 3061,\n",
       " 2,\n",
       " 12065,\n",
       " 734,\n",
       " 4780,\n",
       " 6611,\n",
       " 4,\n",
       " 14675,\n",
       " 27,\n",
       " 450,\n",
       " 485,\n",
       " 24253,\n",
       " 198,\n",
       " 295,\n",
       " 948,\n",
       " 4,\n",
       " 20257,\n",
       " 20429,\n",
       " 1,\n",
       " 55145,\n",
       " 300,\n",
       " 6,\n",
       " 24253,\n",
       " 20429,\n",
       " 16472,\n",
       " 0,\n",
       " 30336,\n",
       " 1,\n",
       " 0,\n",
       " 93,\n",
       " 44,\n",
       " 3835,\n",
       " 2,\n",
       " 55564,\n",
       " 2,\n",
       " 3815,\n",
       " 0,\n",
       " 3414,\n",
       " 1,\n",
       " 0,\n",
       " 1776,\n",
       " 187,\n",
       " 1,\n",
       " 0,\n",
       " 625,\n",
       " 0,\n",
       " 13190,\n",
       " 1,\n",
       " 3,\n",
       " 21,\n",
       " 90,\n",
       " 122,\n",
       " 284,\n",
       " 25,\n",
       " 243,\n",
       " 232,\n",
       " 6,\n",
       " 31,\n",
       " 436,\n",
       " 26409,\n",
       " 1,\n",
       " 173,\n",
       " 5236,\n",
       " 7706,\n",
       " 2535,\n",
       " 4,\n",
       " 29,\n",
       " 95,\n",
       " 1,\n",
       " 289,\n",
       " 602,\n",
       " 4310,\n",
       " 19,\n",
       " 0,\n",
       " 13190,\n",
       " 16472,\n",
       " 49,\n",
       " 187,\n",
       " 117,\n",
       " 46,\n",
       " 359,\n",
       " 19,\n",
       " 0,\n",
       " 384,\n",
       " 242,\n",
       " 96,\n",
       " 31,\n",
       " 5242,\n",
       " 34,\n",
       " 331,\n",
       " 2723,\n",
       " 18,\n",
       " 0,\n",
       " 838,\n",
       " 1376,\n",
       " 27,\n",
       " 32,\n",
       " 8771,\n",
       " 46,\n",
       " 6470,\n",
       " 34,\n",
       " 3246,\n",
       " 0,\n",
       " 27933,\n",
       " 28,\n",
       " 551,\n",
       " 43987,\n",
       " 39,\n",
       " 30,\n",
       " 127,\n",
       " 54107,\n",
       " 435,\n",
       " 84,\n",
       " 0,\n",
       " 64,\n",
       " 1,\n",
       " 0,\n",
       " 102,\n",
       " 462,\n",
       " 83,\n",
       " 2,\n",
       " 25,\n",
       " 232,\n",
       " 18,\n",
       " 47,\n",
       " 11,\n",
       " 26409,\n",
       " 1,\n",
       " 173,\n",
       " 5236,\n",
       " 4,\n",
       " 0,\n",
       " 173,\n",
       " 839,\n",
       " 0,\n",
       " 45,\n",
       " 6,\n",
       " 89,\n",
       " 0,\n",
       " 194,\n",
       " 6,\n",
       " 1014,\n",
       " 1043,\n",
       " 42,\n",
       " 69,\n",
       " 4867,\n",
       " 17,\n",
       " 937,\n",
       " 14924,\n",
       " 4571,\n",
       " 164,\n",
       " 4,\n",
       " 29,\n",
       " 41851,\n",
       " 11888,\n",
       " 23177,\n",
       " 259,\n",
       " 1225,\n",
       " 59081,\n",
       " 3,\n",
       " 22,\n",
       " 7,\n",
       " 16,\n",
       " 99,\n",
       " 35,\n",
       " 628,\n",
       " 0,\n",
       " 2662,\n",
       " 63,\n",
       " 371,\n",
       " 33,\n",
       " 48,\n",
       " 75,\n",
       " 93,\n",
       " 687,\n",
       " 14894,\n",
       " 4049,\n",
       " 28,\n",
       " 818,\n",
       " 917,\n",
       " 11,\n",
       " 101,\n",
       " 4,\n",
       " 11090,\n",
       " 2535,\n",
       " 362,\n",
       " 5,\n",
       " 6073,\n",
       " 2,\n",
       " 663,\n",
       " 4,\n",
       " 0,\n",
       " 63,\n",
       " 701,\n",
       " 435,\n",
       " 40,\n",
       " 4223,\n",
       " 1290,\n",
       " 19,\n",
       " 35,\n",
       " 10,\n",
       " 30,\n",
       " 4780,\n",
       " 2,\n",
       " 94,\n",
       " 25,\n",
       " 49,\n",
       " 29,\n",
       " 5232,\n",
       " 4,\n",
       " 3,\n",
       " 22,\n",
       " 8,\n",
       " 16,\n",
       " 4,\n",
       " 0,\n",
       " 5351,\n",
       " 1,\n",
       " 0,\n",
       " 150,\n",
       " 854,\n",
       " 469,\n",
       " 12305,\n",
       " 334,\n",
       " 30,\n",
       " 17242,\n",
       " 2434,\n",
       " 200,\n",
       " 1293,\n",
       " 141,\n",
       " 12305,\n",
       " 215,\n",
       " 37,\n",
       " 89,\n",
       " 0,\n",
       " 247,\n",
       " 5236,\n",
       " 55,\n",
       " 116,\n",
       " 7095,\n",
       " 38,\n",
       " 1206,\n",
       " 32,\n",
       " 197,\n",
       " 11,\n",
       " 0,\n",
       " 45,\n",
       " 181,\n",
       " 4780,\n",
       " 529,\n",
       " 2,\n",
       " 12305,\n",
       " 11,\n",
       " 0,\n",
       " 1835,\n",
       " 1,\n",
       " 1915,\n",
       " 5236,\n",
       " 41,\n",
       " 34,\n",
       " 32,\n",
       " 228,\n",
       " 75,\n",
       " 4780,\n",
       " 435,\n",
       " 733,\n",
       " 2399,\n",
       " 2,\n",
       " 0,\n",
       " 194,\n",
       " 17,\n",
       " 86,\n",
       " 1073,\n",
       " 11,\n",
       " 30,\n",
       " 14110,\n",
       " 30830,\n",
       " 18,\n",
       " 0,\n",
       " 12079,\n",
       " 63108,\n",
       " 34,\n",
       " 50,\n",
       " 2422,\n",
       " 616,\n",
       " 4,\n",
       " 0,\n",
       " 150,\n",
       " 854,\n",
       " 0,\n",
       " 45,\n",
       " 567,\n",
       " 8752,\n",
       " 4780,\n",
       " 2937,\n",
       " 1034,\n",
       " 14548,\n",
       " 26,\n",
       " 10,\n",
       " 636,\n",
       " 359,\n",
       " 19,\n",
       " 26,\n",
       " 5385,\n",
       " 154,\n",
       " 191,\n",
       " 2937,\n",
       " 1034,\n",
       " 14548,\n",
       " 334,\n",
       " 153,\n",
       " 10,\n",
       " 917,\n",
       " 4,\n",
       " 3,\n",
       " 12,\n",
       " 20,\n",
       " 7,\n",
       " 19,\n",
       " 0,\n",
       " 194,\n",
       " 4780,\n",
       " 17,\n",
       " 989,\n",
       " 11,\n",
       " 5,\n",
       " 567,\n",
       " 1606,\n",
       " 26,\n",
       " 10,\n",
       " 13,\n",
       " 32,\n",
       " 890,\n",
       " 19,\n",
       " 47,\n",
       " 848,\n",
       " 14548,\n",
       " 11,\n",
       " 0,\n",
       " 1835,\n",
       " 1,\n",
       " 173,\n",
       " 4780,\n",
       " 208,\n",
       " 4,\n",
       " 153,\n",
       " 10,\n",
       " 917,\n",
       " 14548,\n",
       " 5470,\n",
       " 23,\n",
       " 0,\n",
       " 419,\n",
       " 13019,\n",
       " 917,\n",
       " 10,\n",
       " 6886,\n",
       " 4,\n",
       " 32,\n",
       " 139,\n",
       " 35,\n",
       " 1346,\n",
       " 0,\n",
       " 2877,\n",
       " 1,\n",
       " 15132,\n",
       " 917,\n",
       " 154,\n",
       " 99,\n",
       " 3466,\n",
       " 38,\n",
       " 652,\n",
       " 445,\n",
       " 6,\n",
       " 89,\n",
       " 2,\n",
       " 3135,\n",
       " 43,\n",
       " 917,\n",
       " 11,\n",
       " 46,\n",
       " 3593,\n",
       " 54,\n",
       " 11,\n",
       " 15368,\n",
       " 1555,\n",
       " 13,\n",
       " 2968,\n",
       " 4,\n",
       " 44,\n",
       " 258,\n",
       " 14548,\n",
       " 1165,\n",
       " 153,\n",
       " 35,\n",
       " 88,\n",
       " 3227,\n",
       " 1131,\n",
       " 52,\n",
       " 38,\n",
       " 751,\n",
       " 445,\n",
       " 6,\n",
       " 89,\n",
       " 1049,\n",
       " 522,\n",
       " 2,\n",
       " 1791,\n",
       " 4,\n",
       " 5813,\n",
       " 23,\n",
       " 1556,\n",
       " 1,\n",
       " 5128,\n",
       " 2,\n",
       " 1293,\n",
       " 14548,\n",
       " 14,\n",
       " 2655,\n",
       " 1,\n",
       " 11090,\n",
       " 33,\n",
       " 35,\n",
       " 88,\n",
       " 29549,\n",
       " 967,\n",
       " 30,\n",
       " 1334,\n",
       " 518,\n",
       " 99,\n",
       " 1131,\n",
       " 2,\n",
       " 308,\n",
       " 159,\n",
       " 512,\n",
       " 0,\n",
       " 807,\n",
       " 1,\n",
       " 43,\n",
       " 1258,\n",
       " 218,\n",
       " 1258,\n",
       " 1037,\n",
       " 33,\n",
       " 1391,\n",
       " 0,\n",
       " 908,\n",
       " 1,\n",
       " 741,\n",
       " 64,\n",
       " 967,\n",
       " 4,\n",
       " 442,\n",
       " 32,\n",
       " 73,\n",
       " 2913,\n",
       " 19,\n",
       " 75,\n",
       " 3,\n",
       " 73,\n",
       " 2968,\n",
       " 27,\n",
       " 0,\n",
       " 1258,\n",
       " 1,\n",
       " 275,\n",
       " 1555,\n",
       " 159,\n",
       " 4320,\n",
       " 2519,\n",
       " 498,\n",
       " 4,\n",
       " 727,\n",
       " 10660,\n",
       " 18946,\n",
       " 30,\n",
       " 762,\n",
       " 246,\n",
       " 986,\n",
       " 73,\n",
       " 31,\n",
       " 177,\n",
       " 97,\n",
       " 6,\n",
       " 740,\n",
       " 3274,\n",
       " 23,\n",
       " 912,\n",
       " 6,\n",
       " 0,\n",
       " 362,\n",
       " 1,\n",
       " 442,\n",
       " 14548,\n",
       " 14,\n",
       " 948,\n",
       " 39,\n",
       " 2025,\n",
       " 222,\n",
       " 150,\n",
       " 741,\n",
       " 476,\n",
       " 1818,\n",
       " 2,\n",
       " 29,\n",
       " 2986,\n",
       " 39,\n",
       " 1022,\n",
       " 4,\n",
       " 0,\n",
       " 854,\n",
       " 1,\n",
       " 3,\n",
       " 12,\n",
       " 20,\n",
       " 12,\n",
       " 4,\n",
       " 302,\n",
       " 14548,\n",
       " 14,\n",
       " 602,\n",
       " 1,\n",
       " 917,\n",
       " 10,\n",
       " 647,\n",
       " 26,\n",
       " 17,\n",
       " 347,\n",
       " 4,\n",
       " 5,\n",
       " 112,\n",
       " 1,\n",
       " 311,\n",
       " 79,\n",
       " 29,\n",
       " 3573,\n",
       " 2,\n",
       " 59,\n",
       " 25,\n",
       " 6448,\n",
       " 4198,\n",
       " 1,\n",
       " 47,\n",
       " 1,\n",
       " 29,\n",
       " 948,\n",
       " 13,\n",
       " 50,\n",
       " 2159,\n",
       " 1590,\n",
       " 67,\n",
       " 621,\n",
       " 2750,\n",
       " 26316,\n",
       " 14,\n",
       " 15763,\n",
       " 4,\n",
       " 29,\n",
       " 0,\n",
       " 10503,\n",
       " 2,\n",
       " 44,\n",
       " 199,\n",
       " 26316,\n",
       " 1461,\n",
       " 19,\n",
       " 51,\n",
       " 636,\n",
       " 1046,\n",
       " 344,\n",
       " 1775,\n",
       " 133,\n",
       " 0,\n",
       " 2583,\n",
       " 1,\n",
       " 93,\n",
       " 917,\n",
       " 11,\n",
       " 5,\n",
       " 244,\n",
       " 389,\n",
       " 445,\n",
       " 4,\n",
       " 152,\n",
       " 2,\n",
       " 0,\n",
       " 147,\n",
       " 2583,\n",
       " 1,\n",
       " 371,\n",
       " 39,\n",
       " 4593,\n",
       " 20968,\n",
       " 28,\n",
       " 9228,\n",
       " 4,\n",
       " 0,\n",
       " 940,\n",
       " 1975,\n",
       " 1,\n",
       " 371,\n",
       " 19,\n",
       " 0,\n",
       " 1131,\n",
       " 25,\n",
       " 44,\n",
       " 1729,\n",
       " 35,\n",
       " 5213,\n",
       " 15763,\n",
       " 2,\n",
       " 5,\n",
       " 143,\n",
       " 1,\n",
       " 4,\n",
       " 33,\n",
       " 1131,\n",
       " 73,\n",
       " 9753,\n",
       " 4,\n",
       " 5829,\n",
       " 1,\n",
       " 51794,\n",
       " 66,\n",
       " 61,\n",
       " 26,\n",
       " 17,\n",
       " 4,\n",
       " 43,\n",
       " 567,\n",
       " 762,\n",
       " 6,\n",
       " 175,\n",
       " 94,\n",
       " 13,\n",
       " 118,\n",
       " 917,\n",
       " 684,\n",
       " 1146,\n",
       " 78,\n",
       " 123,\n",
       " 507,\n",
       " 13078,\n",
       " 5567,\n",
       " 310,\n",
       " 6,\n",
       " 369,\n",
       " 6,\n",
       " 4117,\n",
       " 0,\n",
       " 1797,\n",
       " 6,\n",
       " 118,\n",
       " 5465,\n",
       " 917,\n",
       " 2,\n",
       " 153,\n",
       " 71,\n",
       " 38,\n",
       " 4,\n",
       " 608,\n",
       " 183,\n",
       " 19,\n",
       " 10,\n",
       " 608,\n",
       " 199,\n",
       " 94,\n",
       " 184,\n",
       " 11,\n",
       " 71,\n",
       " 6209,\n",
       " 9039,\n",
       " 11,\n",
       " 7195,\n",
       " 71,\n",
       " 1225,\n",
       " 0,\n",
       " 33814,\n",
       " 1,\n",
       " 0,\n",
       " 1797,\n",
       " 26316,\n",
       " 363,\n",
       " 88,\n",
       " 391,\n",
       " 30,\n",
       " 4780,\n",
       " 35,\n",
       " 1046,\n",
       " 66,\n",
       " 0,\n",
       " 2759,\n",
       " 43300,\n",
       " 2069,\n",
       " 29,\n",
       " 948,\n",
       " 39,\n",
       " 2025,\n",
       " 24,\n",
       " 55,\n",
       " 9936,\n",
       " 7095,\n",
       " 141,\n",
       " 4198,\n",
       " 1,\n",
       " 29,\n",
       " 501,\n",
       " 25,\n",
       " 3237,\n",
       " 63,\n",
       " 10283,\n",
       " 5236,\n",
       " 3244,\n",
       " 9612,\n",
       " 4,\n",
       " 3,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we'll discard it with probability given by \n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.\n",
    "\n",
    "I'm going to leave this up to you as an exercise. This is more of a programming challenge, than about deep learning specifically. But, being able to prepare your data for your network is an important skill to have. Check out my solution to see how I did it.\n",
    "\n",
    "> **Exercise:** Implement subsampling for the words in `int_words`. That is, go through `int_words` and discard each word given the probablility $P(w_i)$ shown above. Note that $P(w_i)$ is the probability that a word is discarded. Assign the subsampled data to `train_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here\n",
    "from collections import Counter\n",
    "trhd = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs = {word: count / total_count for word, count in word_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_drop = {word: 1 - np.sqrt(trhd / freq) for word, freq in freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The final subsampled word list\n",
    "import random\n",
    "train_words = [word for word in int_words if p_drop[word] < random.random()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4626072"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5236,\n",
       " 3082,\n",
       " 11,\n",
       " 741,\n",
       " 10600,\n",
       " 27933,\n",
       " 15154,\n",
       " 62840,\n",
       " 194,\n",
       " 10782,\n",
       " 1325,\n",
       " 58,\n",
       " 3677,\n",
       " 1423,\n",
       " 2759,\n",
       " 7095,\n",
       " 5236,\n",
       " 45588,\n",
       " 2880,\n",
       " 5236,\n",
       " 200,\n",
       " 9021,\n",
       " 6448,\n",
       " 4198,\n",
       " 1137,\n",
       " 447,\n",
       " 6762,\n",
       " 7594,\n",
       " 1775,\n",
       " 93,\n",
       " 11090,\n",
       " 7095,\n",
       " 5958,\n",
       " 4867,\n",
       " 20325,\n",
       " 57859,\n",
       " 25868,\n",
       " 7594,\n",
       " 258,\n",
       " 7594,\n",
       " 1576,\n",
       " 15297,\n",
       " 332,\n",
       " 7095,\n",
       " 4871,\n",
       " 4110,\n",
       " 1131,\n",
       " 4342,\n",
       " 8129,\n",
       " 153,\n",
       " 7095,\n",
       " 1423,\n",
       " 8190,\n",
       " 153,\n",
       " 4160,\n",
       " 948,\n",
       " 310,\n",
       " 364,\n",
       " 1821,\n",
       " 8426,\n",
       " 371,\n",
       " 753,\n",
       " 1735,\n",
       " 8079,\n",
       " 24253,\n",
       " 2,\n",
       " 224,\n",
       " 4780,\n",
       " 8134,\n",
       " 1467,\n",
       " 24253,\n",
       " 12800,\n",
       " 6130,\n",
       " 4196,\n",
       " 21164,\n",
       " 2431,\n",
       " 16569,\n",
       " 7247,\n",
       " 1194,\n",
       " 10228,\n",
       " 15132,\n",
       " 1123,\n",
       " 1049,\n",
       " 12305,\n",
       " 7095,\n",
       " 11090,\n",
       " 12065,\n",
       " 4780,\n",
       " 6611,\n",
       " 14675,\n",
       " 24253,\n",
       " 20257,\n",
       " 20429,\n",
       " 55145,\n",
       " 300,\n",
       " 24253,\n",
       " 20429,\n",
       " 16472,\n",
       " 30336,\n",
       " 3835,\n",
       " 55564,\n",
       " 3815,\n",
       " 3414,\n",
       " 1776,\n",
       " 625,\n",
       " 13190,\n",
       " 122,\n",
       " 436,\n",
       " 26409,\n",
       " 7706,\n",
       " 289,\n",
       " 602,\n",
       " 4310,\n",
       " 13190,\n",
       " 16472,\n",
       " 359,\n",
       " 384,\n",
       " 242,\n",
       " 5242,\n",
       " 838,\n",
       " 1376,\n",
       " 8771,\n",
       " 6470,\n",
       " 27933,\n",
       " 43987,\n",
       " 54107,\n",
       " 232,\n",
       " 26409,\n",
       " 5236,\n",
       " 1043,\n",
       " 4867,\n",
       " 937,\n",
       " 14924,\n",
       " 4571,\n",
       " 41851,\n",
       " 11888,\n",
       " 23177,\n",
       " 1225,\n",
       " 59081,\n",
       " 16,\n",
       " 14894,\n",
       " 4049,\n",
       " 818,\n",
       " 101,\n",
       " 11090,\n",
       " 2535,\n",
       " 6073,\n",
       " 663,\n",
       " 4223,\n",
       " 1290,\n",
       " 4780,\n",
       " 49,\n",
       " 5351,\n",
       " 12305,\n",
       " 17242,\n",
       " 12305,\n",
       " 89,\n",
       " 5236,\n",
       " 7095,\n",
       " 181,\n",
       " 4780,\n",
       " 529,\n",
       " 12305,\n",
       " 1835,\n",
       " 1915,\n",
       " 5236,\n",
       " 75,\n",
       " 4780,\n",
       " 733,\n",
       " 14110,\n",
       " 30830,\n",
       " 12079,\n",
       " 63108,\n",
       " 854,\n",
       " 567,\n",
       " 8752,\n",
       " 2937,\n",
       " 14548,\n",
       " 19,\n",
       " 5385,\n",
       " 2937,\n",
       " 14548,\n",
       " 153,\n",
       " 917,\n",
       " 567,\n",
       " 890,\n",
       " 19,\n",
       " 14548,\n",
       " 1835,\n",
       " 208,\n",
       " 14548,\n",
       " 5470,\n",
       " 13019,\n",
       " 917,\n",
       " 6886,\n",
       " 2877,\n",
       " 15132,\n",
       " 3466,\n",
       " 3135,\n",
       " 3593,\n",
       " 15368,\n",
       " 14548,\n",
       " 153,\n",
       " 3227,\n",
       " 751,\n",
       " 1791,\n",
       " 5813,\n",
       " 1556,\n",
       " 14548,\n",
       " 2655,\n",
       " 11090,\n",
       " 29549,\n",
       " 518,\n",
       " 1037,\n",
       " 2968,\n",
       " 1258,\n",
       " 1555,\n",
       " 4320,\n",
       " 2519,\n",
       " 10660,\n",
       " 18946,\n",
       " 740,\n",
       " 14548,\n",
       " 2025,\n",
       " 2986,\n",
       " 14548,\n",
       " 10,\n",
       " 647,\n",
       " 6448,\n",
       " 1590,\n",
       " 2750,\n",
       " 26316,\n",
       " 15763,\n",
       " 10503,\n",
       " 26316,\n",
       " 51,\n",
       " 636,\n",
       " 1046,\n",
       " 344,\n",
       " 1775,\n",
       " 2583,\n",
       " 389,\n",
       " 445,\n",
       " 20968,\n",
       " 9228,\n",
       " 940,\n",
       " 1975,\n",
       " 1729,\n",
       " 5213,\n",
       " 15763,\n",
       " 1131,\n",
       " 9753,\n",
       " 5829,\n",
       " 51794,\n",
       " 762,\n",
       " 507,\n",
       " 13078,\n",
       " 5567,\n",
       " 369,\n",
       " 4117,\n",
       " 1797,\n",
       " 917,\n",
       " 183,\n",
       " 6209,\n",
       " 9039,\n",
       " 7195,\n",
       " 33814,\n",
       " 1797,\n",
       " 26316,\n",
       " 4780,\n",
       " 1046,\n",
       " 2759,\n",
       " 43300,\n",
       " 2025,\n",
       " 9936,\n",
       " 7095,\n",
       " 4198,\n",
       " 10283,\n",
       " 5236,\n",
       " 9612,\n",
       " 9,\n",
       " 10877,\n",
       " 54072,\n",
       " 2410,\n",
       " 617,\n",
       " 7423,\n",
       " 5103,\n",
       " 33,\n",
       " 17194,\n",
       " 1183,\n",
       " 1217,\n",
       " 5858,\n",
       " 7516,\n",
       " 1130,\n",
       " 625,\n",
       " 1130,\n",
       " 5858,\n",
       " 7440,\n",
       " 18914,\n",
       " 4780,\n",
       " 5764,\n",
       " 15266,\n",
       " 5858,\n",
       " 2,\n",
       " 334,\n",
       " 5630,\n",
       " 16481,\n",
       " 9612,\n",
       " 5858,\n",
       " 46728,\n",
       " 16748,\n",
       " 3138,\n",
       " 6526,\n",
       " 9612,\n",
       " 3415,\n",
       " 10877,\n",
       " 5858,\n",
       " 469,\n",
       " 12530,\n",
       " 2664,\n",
       " 7,\n",
       " 26,\n",
       " 7097,\n",
       " 10283,\n",
       " 4780,\n",
       " 16481,\n",
       " 676,\n",
       " 0,\n",
       " 9612,\n",
       " 4546,\n",
       " 10283,\n",
       " 5236,\n",
       " 6292,\n",
       " 12530,\n",
       " 4702,\n",
       " 5858,\n",
       " 24,\n",
       " 30456,\n",
       " 1194,\n",
       " 14548,\n",
       " 606,\n",
       " 26316,\n",
       " 15763,\n",
       " 3381,\n",
       " 7304,\n",
       " 1123,\n",
       " 9612,\n",
       " 2128,\n",
       " 1165,\n",
       " 4160,\n",
       " 246,\n",
       " 8618,\n",
       " 1709,\n",
       " 6088,\n",
       " 2913,\n",
       " 21666,\n",
       " 26545,\n",
       " 24207,\n",
       " 2358,\n",
       " 5334,\n",
       " 9246,\n",
       " 4089,\n",
       " 193,\n",
       " 6781,\n",
       " 19112,\n",
       " 6153,\n",
       " 1547,\n",
       " 9661,\n",
       " 62409,\n",
       " 14,\n",
       " 1893,\n",
       " 6779,\n",
       " 5236,\n",
       " 6704,\n",
       " 1022,\n",
       " 1555,\n",
       " 134,\n",
       " 2980,\n",
       " 193,\n",
       " 668,\n",
       " 10929,\n",
       " 8960,\n",
       " 32976,\n",
       " 6781,\n",
       " 19112,\n",
       " 12,\n",
       " 45,\n",
       " 43173,\n",
       " 2980,\n",
       " 19112,\n",
       " 4681,\n",
       " 32885,\n",
       " 1298,\n",
       " 3281,\n",
       " 2281,\n",
       " 7095,\n",
       " 19112,\n",
       " 8311,\n",
       " 24253,\n",
       " 14,\n",
       " 11434,\n",
       " 2708,\n",
       " 18196,\n",
       " 200,\n",
       " 16479,\n",
       " 29,\n",
       " 19112,\n",
       " 8840,\n",
       " 7594,\n",
       " 4906,\n",
       " 4994,\n",
       " 1511,\n",
       " 196,\n",
       " 97,\n",
       " 1870,\n",
       " 12,\n",
       " 58031,\n",
       " 11700,\n",
       " 945,\n",
       " 3079,\n",
       " 847,\n",
       " 7095,\n",
       " 11942,\n",
       " 2723,\n",
       " 969,\n",
       " 6073,\n",
       " 4378,\n",
       " 48,\n",
       " 1486,\n",
       " 4628,\n",
       " 3652,\n",
       " 28671,\n",
       " 3246,\n",
       " 24253,\n",
       " 14548,\n",
       " 19112,\n",
       " 19704,\n",
       " 32976,\n",
       " 7095,\n",
       " 726,\n",
       " 19112,\n",
       " 2856,\n",
       " 21211,\n",
       " 8421,\n",
       " 1117,\n",
       " 442,\n",
       " 279,\n",
       " 6158,\n",
       " 31,\n",
       " 1034,\n",
       " 45,\n",
       " 397,\n",
       " 391,\n",
       " 6073,\n",
       " 14548,\n",
       " 1461,\n",
       " 1,\n",
       " 1258,\n",
       " 5478,\n",
       " 40,\n",
       " 10575,\n",
       " 3543,\n",
       " 68,\n",
       " 543,\n",
       " 250,\n",
       " 1392,\n",
       " 24253,\n",
       " 11841,\n",
       " 9691,\n",
       " 332,\n",
       " 4682,\n",
       " 7887,\n",
       " 18946,\n",
       " 1967,\n",
       " 7505,\n",
       " 4400,\n",
       " 4342,\n",
       " 1989,\n",
       " 1159,\n",
       " 4780,\n",
       " 4050,\n",
       " 8200,\n",
       " 11015,\n",
       " 899,\n",
       " 30611,\n",
       " 28941,\n",
       " 1818,\n",
       " 4780,\n",
       " 3246,\n",
       " 4054,\n",
       " 3500,\n",
       " 49498,\n",
       " 30674,\n",
       " 6800,\n",
       " 13271,\n",
       " 10825,\n",
       " 3246,\n",
       " 5236,\n",
       " 142,\n",
       " 10283,\n",
       " 7095,\n",
       " 7774,\n",
       " 3138,\n",
       " 3244,\n",
       " 9612,\n",
       " 3581,\n",
       " 36281,\n",
       " 24253,\n",
       " 1117,\n",
       " 5236,\n",
       " 8437,\n",
       " 5236,\n",
       " 4384,\n",
       " 13381,\n",
       " 13324,\n",
       " 4871,\n",
       " 7095,\n",
       " 3778,\n",
       " 2873,\n",
       " 15916,\n",
       " 40098,\n",
       " 5101,\n",
       " 7095,\n",
       " 11508,\n",
       " 3171,\n",
       " 1190,\n",
       " 1666,\n",
       " 10068,\n",
       " 15916,\n",
       " 2666,\n",
       " 93,\n",
       " 5236,\n",
       " 1522,\n",
       " 88,\n",
       " 4384,\n",
       " 13381,\n",
       " 13324,\n",
       " 4338,\n",
       " 1665,\n",
       " 30854,\n",
       " 2,\n",
       " 10108,\n",
       " 9858,\n",
       " 15431,\n",
       " 17697,\n",
       " 13133,\n",
       " 3178,\n",
       " 4,\n",
       " 2788,\n",
       " 5101,\n",
       " 13346,\n",
       " 17576,\n",
       " 9031,\n",
       " 5363,\n",
       " 6781,\n",
       " 19112,\n",
       " 46896,\n",
       " 39151,\n",
       " 1666,\n",
       " 7902,\n",
       " 1893,\n",
       " 5364,\n",
       " 8248,\n",
       " 39151,\n",
       " 4,\n",
       " 19112,\n",
       " 17992,\n",
       " 7095,\n",
       " 13832,\n",
       " 7095,\n",
       " 5213,\n",
       " 652,\n",
       " 23523,\n",
       " 11647,\n",
       " 602,\n",
       " 14533,\n",
       " 21127,\n",
       " 1194,\n",
       " 335,\n",
       " 7095,\n",
       " 5549,\n",
       " 24627,\n",
       " 5549,\n",
       " 24627,\n",
       " 90,\n",
       " 7335,\n",
       " 5478,\n",
       " 435,\n",
       " 5646,\n",
       " 1522,\n",
       " 2529,\n",
       " 2281,\n",
       " 31404,\n",
       " 724,\n",
       " 4780,\n",
       " 4050,\n",
       " 24627,\n",
       " 60,\n",
       " 22,\n",
       " 10944,\n",
       " 5236,\n",
       " 59151,\n",
       " 13055,\n",
       " 308,\n",
       " 3992,\n",
       " 25348,\n",
       " 15928,\n",
       " 22014,\n",
       " 62474,\n",
       " 56230,\n",
       " 181,\n",
       " 5549,\n",
       " 28941,\n",
       " 435,\n",
       " 13726,\n",
       " 13726,\n",
       " 56230,\n",
       " 6073,\n",
       " 3246,\n",
       " 2241,\n",
       " 2529,\n",
       " 60,\n",
       " 56230,\n",
       " 817,\n",
       " 5549,\n",
       " 24627,\n",
       " 26317,\n",
       " 24627,\n",
       " 435,\n",
       " 435,\n",
       " 803,\n",
       " 900,\n",
       " 1166,\n",
       " 1555,\n",
       " 40524,\n",
       " 737,\n",
       " 15,\n",
       " 250,\n",
       " 15636,\n",
       " 29829,\n",
       " 2122,\n",
       " 2529,\n",
       " 16322,\n",
       " 16879,\n",
       " 2399,\n",
       " 314,\n",
       " 3617,\n",
       " 1009,\n",
       " 37697,\n",
       " 15574,\n",
       " 31404,\n",
       " 30674,\n",
       " 4384,\n",
       " 532,\n",
       " 6946,\n",
       " 3640,\n",
       " 4329,\n",
       " 3203,\n",
       " 567,\n",
       " 4780,\n",
       " 512,\n",
       " 20587,\n",
       " 22,\n",
       " 53458,\n",
       " 10223,\n",
       " 56665,\n",
       " 30674,\n",
       " 3,\n",
       " 1294,\n",
       " 30674,\n",
       " 546,\n",
       " 30674,\n",
       " 4,\n",
       " 546,\n",
       " 5236,\n",
       " 803,\n",
       " 31404,\n",
       " 11503,\n",
       " 17002,\n",
       " 19114,\n",
       " 1568,\n",
       " 5033,\n",
       " 32575,\n",
       " 3890,\n",
       " 2333,\n",
       " 1818,\n",
       " 2822,\n",
       " 8,\n",
       " 30674,\n",
       " 5549,\n",
       " 28941,\n",
       " 5549,\n",
       " 24627,\n",
       " 1772,\n",
       " 11,\n",
       " 972,\n",
       " 915,\n",
       " 5842,\n",
       " 53458,\n",
       " 2245,\n",
       " 56665,\n",
       " 30674,\n",
       " 56230,\n",
       " 2201,\n",
       " 97,\n",
       " 638,\n",
       " 339,\n",
       " 920,\n",
       " 28941,\n",
       " 10749,\n",
       " 10749,\n",
       " 2474,\n",
       " 1893,\n",
       " 18170,\n",
       " 1555,\n",
       " 70,\n",
       " 1273,\n",
       " 5549,\n",
       " 24627,\n",
       " 29829,\n",
       " 5523,\n",
       " 6,\n",
       " 1380,\n",
       " 12058,\n",
       " 54,\n",
       " 5549,\n",
       " 24627,\n",
       " 4338,\n",
       " 13568,\n",
       " 1576,\n",
       " 14785,\n",
       " 31404,\n",
       " 18785,\n",
       " 1556,\n",
       " 10749,\n",
       " 1172,\n",
       " 554,\n",
       " 554,\n",
       " 22,\n",
       " 18498,\n",
       " 312,\n",
       " 5236,\n",
       " 7095,\n",
       " 5435,\n",
       " 8850,\n",
       " 595,\n",
       " 547,\n",
       " 9661,\n",
       " 7095,\n",
       " 8442,\n",
       " 8850,\n",
       " 7095,\n",
       " 1217,\n",
       " 9757,\n",
       " 43928,\n",
       " 7095,\n",
       " 39,\n",
       " 5854,\n",
       " 8834,\n",
       " 8850,\n",
       " 4,\n",
       " 4517,\n",
       " 2239,\n",
       " 8745,\n",
       " 8850,\n",
       " 8208,\n",
       " 336,\n",
       " 26680,\n",
       " 51678,\n",
       " 4969,\n",
       " 7095,\n",
       " 8200,\n",
       " 11015,\n",
       " 30611,\n",
       " 151,\n",
       " 2815,\n",
       " 45806,\n",
       " 8442,\n",
       " 738,\n",
       " 8039,\n",
       " 43928,\n",
       " 4772,\n",
       " 2127,\n",
       " 3506,\n",
       " 11639,\n",
       " 6,\n",
       " 14273,\n",
       " 8442,\n",
       " 19112,\n",
       " 7939,\n",
       " 3692,\n",
       " 4994,\n",
       " 1507,\n",
       " 8850,\n",
       " 547,\n",
       " 554,\n",
       " 4780,\n",
       " 5154,\n",
       " 797,\n",
       " 8442,\n",
       " 2077,\n",
       " 1117,\n",
       " 805,\n",
       " 1779,\n",
       " 5759,\n",
       " 5236,\n",
       " 2349,\n",
       " 1818,\n",
       " 28941,\n",
       " 1818,\n",
       " 56230,\n",
       " 40524,\n",
       " 46220,\n",
       " 571,\n",
       " 4780,\n",
       " 14642,\n",
       " 26680,\n",
       " 51678,\n",
       " 3714,\n",
       " 7095,\n",
       " 1142,\n",
       " 1576,\n",
       " 26317,\n",
       " 6800,\n",
       " 26757,\n",
       " 2800,\n",
       " 6073,\n",
       " 4050,\n",
       " 1117,\n",
       " 7095,\n",
       " 1346,\n",
       " 2800,\n",
       " 14149,\n",
       " 308,\n",
       " 694,\n",
       " 30,\n",
       " 4780,\n",
       " 1556,\n",
       " 3203,\n",
       " 6482,\n",
       " 3203,\n",
       " 3058,\n",
       " 16339,\n",
       " 211,\n",
       " 1555,\n",
       " 10749,\n",
       " 4,\n",
       " 711,\n",
       " 531,\n",
       " 4780,\n",
       " 328,\n",
       " 2474,\n",
       " 4780,\n",
       " 7537,\n",
       " 85,\n",
       " 10143,\n",
       " 1785,\n",
       " 30674,\n",
       " 5100,\n",
       " 6738,\n",
       " 18801,\n",
       " 7887,\n",
       " 3115,\n",
       " 5551,\n",
       " 5236,\n",
       " 4975,\n",
       " 7095,\n",
       " 3202,\n",
       " 6983,\n",
       " 279,\n",
       " 14920,\n",
       " 19504,\n",
       " 3448,\n",
       " 176,\n",
       " 4748,\n",
       " 1979,\n",
       " 11065,\n",
       " 3057,\n",
       " 5445,\n",
       " 5152,\n",
       " 5445,\n",
       " 5445,\n",
       " 7594,\n",
       " 2733,\n",
       " 30573,\n",
       " 21016,\n",
       " 38146,\n",
       " 208,\n",
       " 1411,\n",
       " 18801,\n",
       " 421,\n",
       " 689,\n",
       " 12717,\n",
       " 7095,\n",
       " 926,\n",
       " 738,\n",
       " 803,\n",
       " 30674,\n",
       " 1930,\n",
       " 2519,\n",
       " 2006,\n",
       " 44748,\n",
       " 30674,\n",
       " 2029,\n",
       " 1507,\n",
       " 30674,\n",
       " 947,\n",
       " 738,\n",
       " 4780,\n",
       " 926,\n",
       " 30,\n",
       " 1649,\n",
       " 546,\n",
       " 462,\n",
       " 10289,\n",
       " 435,\n",
       " 6186,\n",
       " 1555,\n",
       " 1416,\n",
       " 12735,\n",
       " 181,\n",
       " 5757,\n",
       " 167,\n",
       " 47511,\n",
       " 5929,\n",
       " 4748,\n",
       " 7095,\n",
       " 3217,\n",
       " 5400,\n",
       " 2708,\n",
       " 30674,\n",
       " 836,\n",
       " 14450,\n",
       " 4056,\n",
       " 16192,\n",
       " 280,\n",
       " 1036,\n",
       " 6868,\n",
       " 32270,\n",
       " 10941,\n",
       " 16374,\n",
       " 11942,\n",
       " 7095,\n",
       " 8,\n",
       " 7095,\n",
       " 1074,\n",
       " 3858,\n",
       " 4748,\n",
       " 351,\n",
       " 7095,\n",
       " 1186,\n",
       " 8361,\n",
       " 4994,\n",
       " 5213,\n",
       " 23697,\n",
       " 18801,\n",
       " 10615,\n",
       " 4119,\n",
       " 527,\n",
       " 9597,\n",
       " 623,\n",
       " 250,\n",
       " 623,\n",
       " 5236,\n",
       " 11647,\n",
       " 4780,\n",
       " 8411,\n",
       " 9056,\n",
       " 17044,\n",
       " 344,\n",
       " 4198,\n",
       " 1676,\n",
       " 5236,\n",
       " 7095,\n",
       " 694,\n",
       " 13164,\n",
       " 867,\n",
       " 113,\n",
       " 3562,\n",
       " 2050,\n",
       " 46536,\n",
       " 15222,\n",
       " 61,\n",
       " 728,\n",
       " 936,\n",
       " 7095,\n",
       " 13461,\n",
       " 13762,\n",
       " 3947,\n",
       " 36728,\n",
       " 51,\n",
       " 4871,\n",
       " 5236,\n",
       " 3374,\n",
       " 11647,\n",
       " 597,\n",
       " 212,\n",
       " 12375,\n",
       " 21127,\n",
       " 7095,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that our data is in good shape, we need to get it into the proper form to pass it into our network. With the skip-gram architecture, for each word in the text, we want to grab all the words in a window around that word, with size $C$. \n",
    "\n",
    "From [Mikolov et al.](https://arxiv.org/pdf/1301.3781.pdf): \n",
    "\n",
    "\"Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples... If we choose $C = 5$, for each training word we will select randomly a number $R$ in range $< 1; C >$, and then use $R$ words from history and $R$ words from the future of the current word as correct labels.\"\n",
    "\n",
    "> **Exercise:** Implement a function `get_target` that receives a list of words, an index, and a window size, then returns a list of words in the window around the index. Make sure to use the algorithm described above, where you choose a random number of words from the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    # Your code here\n",
    "    # select r before and after \n",
    "    R = np.random.randint(1, window_size + 1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R # todo, find out the max index for the words\n",
    "    targetWords = set(words[start:idx] + words[idx + 1: stop + 1])\n",
    "    \n",
    "    return list(targetWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function that returns batches for our network. The idea is that it grabs `batch_size` words from a words list. Then for each of those words, it gets the target words in the window. I haven't found a way to pass in a random number of target words and get it to work with the architecture, so I make one row per input-target pair. This is a generator function by the way, helps save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Building the graph\n",
    "\n",
    "From Chris McCormick's blog, we can see the general structure of our network.\n",
    "![embedding_network](./assets/skip_gram_net_arch.png)\n",
    "\n",
    "The input words are passed in as one-hot encoded vectors. This will go into a hidden layer of linear units, then into a softmax layer. We'll use the softmax layer to make a prediction like normal.\n",
    "\n",
    "The idea here is to train the hidden layer weight matrix to find efficient representations for our words. This weight matrix is usually called the embedding matrix or embedding look-up table. We can discard the softmax layer becuase we don't really care about making predictions with this network. We just want the embedding matrix so we can use it in other networks we build from the dataset.\n",
    "\n",
    "I'm going to have you build the graph in stages now. First off, creating the `inputs` and `labels` placeholders like normal.\n",
    "\n",
    "> **Exercise:** Assign `inputs` and `labels` using `tf.placeholder`. We're going to be passing in integers, so set the data types to `tf.int32`. The batches we're passing in will have varying sizes, so set the batch sizes to [`None`]. To make things work later, you'll need to set the second dimension of `labels` to `None` or `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The embedding matrix has a size of the number of words by the number of neurons in the hidden layer. So, if you have 10,000 words and 300 hidden units, the matrix will have size $10,000 \\times 300$. Remember that we're using one-hot encoded vectors for our inputs. When you do the matrix multiplication of the one-hot vector with the embedding matrix, you end up selecting only one row out of the entire matrix:\n",
    "\n",
    "![one-hot matrix multiplication](assets/matrix_mult_w_one_hot.png)\n",
    "\n",
    "You don't actually need to do the matrix multiplication, you just need to select the row in the embedding matrix that corresponds to the input word. Then, the embedding matrix becomes a lookup table, you're looking up a vector the size of the hidden layer that represents the input word.\n",
    "\n",
    "<img src=\"assets/word2vec_weight_matrix_lookup_table.png\" width=500>\n",
    "\n",
    "\n",
    "> **Exercise:** Tensorflow provides a convenient function [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) that does this lookup for us. You pass in the embedding matrix and a tensor of integers, then it returns rows in the matrix corresponding to those integers. Below, set the number of embedding features you'll use (200 is a good start), create the embedding matrix variable, and use [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) to get the embedding tensors. For the embedding matrix, I suggest you initialize it with a uniform random numbers between -1 and 1 using [tf.random_uniform](https://www.tensorflow.org/api_docs/python/tf/random_uniform). This [TensorFlow tutorial](https://www.tensorflow.org/tutorials/word2vec) will help if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding =  300 # Number of embedding features \n",
    "with train_graph.as_default():\n",
    "    # create embedding weight matrix here\n",
    "    embedding =  tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1)) \n",
    "    \n",
    "    # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every example we give the network, we train it using the output from the softmax layer. That means for each input, we're making very small changes to millions of weights even though we only have one true example. This makes training the network very inefficient. We can approximate the loss from the softmax layer by only updating a small subset of all the weights at once. We'll update the weights for the correct label, but only a small number of incorrect labels. This is called [\"negative sampling\"](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). Tensorflow has a convenient function to do this, [`tf.nn.sampled_softmax_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss).\n",
    "\n",
    "> **Exercise:** Below, create weights and biases for the softmax layer. Then, use [`tf.nn.sampled_softmax_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss) to calculate the loss. Be sure to read the documentation to figure out how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of negative labels to sample\n",
    "n_sampled = 100\n",
    "with train_graph.as_default():\n",
    "    # create softmax weight matrix here\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1)) \n",
    "    # create softmax biases here\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b,\n",
    "                                     labels,\n",
    "                                     embed,\n",
    "                                     n_sampled,\n",
    "                                     n_vocab)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "This code is from Thushan Ganegedara's implementation. Here we're going to choose a few common words and few uncommon words. Then, we'll print out the closest words to them. It's a nice way to check that our embedding table is grouping together words with similar semantic meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below is the code to train the network. Every 100 batches it reports the training loss. Every 1000 batches, it'll print out the validation words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b1d3869cd0d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             feed = {inputs: x,\n\u001b[1;32m     19\u001b[0m                     labels: np.array(y)[:, None]}\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 1000\n",
    "window_size = 10\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 1000 == 0:\n",
    "                ## From Thushan Ganegedara's implementation\n",
    "                # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = int_to_vocab[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = int_to_vocab[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the trained network if you need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "<built-in function TF_Run> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: expected bytes, NoneType found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4eeca6a379f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_graph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0membed_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1437\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1439\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leiming/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function TF_Run> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the word vectors\n",
    "\n",
    "Below we'll use T-SNE to visualize how our high-dimensional word vectors cluster together. T-SNE is used to project these vectors into two dimensions while preserving local stucture. Check out [this post from Christopher Olah](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) to learn more about T-SNE and other ways to visualize high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "viz_words = 500\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlnd-tf-lab]",
   "language": "python",
   "name": "conda-env-dlnd-tf-lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
